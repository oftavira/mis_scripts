{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "'==============================================================================================================================='"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "from urllib import request\n",
    "from time   import sleep\n",
    "from bs4    import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Links for IOP search\n",
    "\n",
    "IOP_S  = \"https://iopscience.iop.org/nsearch?terms=\"\n",
    "IOP_E  = \"&nextPage=2&previousPage=-1&currentPage=\"\n",
    "IOP_P  = \"&searchDatePeriod=anytime&orderBy=relevance&pageLength=50\"\n",
    "\n",
    "# Links for NATURE search\n",
    "\n",
    "NANO_S = 'https://nano.nature.com/search?term=freeText%3A'\n",
    "NANO_E = '&sort-by=relevance&page-number='\n",
    "NANO_P = '&workflow=article'\n",
    "\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "# -format- returns a string that can be readed as a query to IOP or NANO\n",
    "\n",
    "def format(query):\n",
    "    pre_form = query.split(\" \")\n",
    "    return \"+\".join(pre_form)\n",
    "\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "# Some pages may have problems if requested with -urllib- or -requests-, SwitchKitchen is implemented to\n",
    "# switch the way we obtain a BeautifulSoup object\n",
    "\n",
    "\n",
    "def SwitchKitchen(uri, kitchen = \"requests\", parse = \"lxml\"):\n",
    "\n",
    "    # Request with --requests.get(url)--\n",
    "\n",
    "    if   kitchen == \"requests\":\n",
    "         response = requests.get(uri)\n",
    "         soup     = BeautifulSoup(response.text, parse)\n",
    "\n",
    "    elif kitchen == \"urllib\":\n",
    "\n",
    "    # Request with --request.urlopen(url)--\n",
    "\n",
    "         response = request.urlopen(uri)\n",
    "         soup     = BeautifulSoup(response, parse)\n",
    "    else:\n",
    "        raise Exception(\"Problema en la elección del método mara obtener la respuesta del servidor.\")\n",
    "    return soup;\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "# /Souparticles : Returns a list of BeautifulSoup objects that contain articles\n",
    "\n",
    "# /material     : Is a str. containing the search term\n",
    "# /page         : The page in which we are searching\n",
    "# /max_try      : The maximum tries before givin up, commonly this is a result of a 400 status response from the server\n",
    "# /slpy         : Is the time between requests, recommended to prevent overloading requested server\n",
    "\n",
    "\n",
    "def souparticlesIOP(query, page, max_int = 5, slpy = 3):\n",
    "    # At the beginning our IOP Soup is a None object\n",
    "    soupIOP = None\n",
    "    ints  = 0\n",
    "\n",
    "    while ints<max_int:    \n",
    "    # We try to catch some content from the server\n",
    "\n",
    "        try:\n",
    "            uri      = IOP_S+ format(query) + IOP_E + str(page) + IOP_P\n",
    "            # If we catch a 200 response we can make a Beautifoul Soup (BS), parsing with lxml\n",
    "            soupIOP  = SwitchKitchen(uri,kitchen = \"urllib\")\n",
    "            break\n",
    "        except:\n",
    "            # If we catch an error, we can retry\n",
    "            print(\"Reintentando pag: \" + str(page))\n",
    "            sleep(slpy)\n",
    "            pass\n",
    "        finally:\n",
    "            ints +=1\n",
    "    \n",
    "    # For soupIOP == None this process needs to be repeated\n",
    "    if(soupIOP==None):\n",
    "        print( \"Por favor reinicie la busqueda para pag: \" + str(page))\n",
    "        return None;\n",
    "    else:\n",
    "        # Returning the articles in a BS object\n",
    "        print(\"Añadida pag: \" + str(page))\n",
    "        return soupIOP.find_all('div', {\"class\" : \"art-list-item-body\"})\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "def souparticlesNANO(query, page, max_int = 5, slpy = 3):\n",
    "    nanosoup = None\n",
    "    ints=0\n",
    "    while ints<max_int:\n",
    "        try:\n",
    "            uri      = NANO_S+'\"'+format(query)+'\"'+NANO_E+str(page)+NANO_P\n",
    "            nanosoup = SwitchKitchen(uri, kitchen = \"requests\")\n",
    "            break\n",
    "        except:\n",
    "            print(\"Reintentando pag: \" + str(page))\n",
    "            sleep(slpy)\n",
    "            pass\n",
    "        finally:\n",
    "            ints +=1\n",
    "    if(nanosoup==None):\n",
    "        print( \"Por favor reinicie la busqueda para pag: \" + str(page))\n",
    "        return None;\n",
    "    else:\n",
    "        print(\"Añadida pag: \" + str(page))\n",
    "        return nanosoup.find_all('li', {\"class\" : \"Results_listItem\" })\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "# -Pages- returns a list of pages, each element contains the articles within that page\n",
    "\n",
    "def PagesIOP(query):\n",
    "    raw_pages   = []\n",
    "    init_uri  = IOP_S+ format(query) + IOP_E + str(1) + IOP_P\n",
    "    init_soup = SwitchKitchen(init_uri,kitchen = \"urllib\")\n",
    "    end_pages = init_soup.findChild(\"p\",{\"class\":\"pgs small\"}).get_text().split(\" \")[2]\n",
    "    for i in range(1, int(end_pages)+1):\n",
    "        raw_pages.append(souparticlesIOP(query, i))\n",
    "    return raw_pages;\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "def PagesNANO(query, cut = 12):\n",
    "    raw_pages = []\n",
    "    init_uri  = NANO_S+'\"'+format(query)+'\"'+NANO_E+str(1)+NANO_P\n",
    "    init_soup = SwitchKitchen(init_uri,kitchen = \"requests\")\n",
    "    end_pages = init_soup.findChild(\"span\",{\"class\":\"Pagination_numOfPages\"})\n",
    "    num_of_pgs= int(end_pages.get_text())\n",
    "    if num_of_pgs<12:\n",
    "        for i in range(1,num_of_pgs+1):\n",
    "            raw_pages.append(souparticlesNANO(query, i))\n",
    "    else:\n",
    "        for i in range(1, cut + 1 ):\n",
    "            raw_pages.append(souparticlesNANO(query, i))\n",
    "    return raw_pages;\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "# -stringify- returns a list of articles, each article containing it´s features, that can be readed as normal strings\n",
    "\n",
    "def stringifyIOP(raw_pages):\n",
    "    arts_db = []\n",
    "    for page in raw_pages:\n",
    "        for element in page:\n",
    "            try:\n",
    "                title     =     element.findChild('h2' , {\"class\"   : \"art-list-item-title\"}).findChild(\"a\")          # 0 title\n",
    "                abstract  =     element.findChild('div', {\"class\"   : \"article-text view-text-small\"}).findChild(\"p\") # 1 abstract\n",
    "                DOI       =     element.findChild('a'  , {\"class\"   : \"mr-2\"})                                        # 2 DOI\n",
    "                journal   =     element.findChild('em')                                                               # 3 Journal\n",
    "                #vol       =     element.findChild('b')                                                              \n",
    "                authors   =     element.find_all('span', {\"itemprop\": \"author\"})                                      # 4 auths\n",
    "\n",
    "                yearfind  =     element.find_all(\"p\"   , { \"class\" :\"small art-list-item-meta\"})                      # 5 year\n",
    "                year      =     int(yearfind[1].text[14:19])\n",
    "\n",
    "                arts_db.append([title.text ,abstract.text, DOI.text, journal.text,[auth.getText() for auth in authors], year])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return arts_db;\n",
    "\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "def stringifyNANO(raw_pages):\n",
    "    arts_db = []\n",
    "    for page in raw_pages:\n",
    "        for element in page:\n",
    "            try:\n",
    "                title       =     element.findChild(\"h2\").getText().replace(\"\\n\",\"\")                                # 0 Title\n",
    "                miscel      =     element.find_all(\"p\")\n",
    "\n",
    "                abstract    =     miscel[1].getText()                                                               # 1 Abstract\n",
    "                DOI         =     element.findChild('div', {\"class\": \"Doi\"}).getText()                              # 2 DOI\n",
    "                journal     =     miscel[0].findChild(\"strong\").getText()                                           # 3 Journal\n",
    "\n",
    "                #cited       =     miscel[2].getText()\n",
    "\n",
    "                authstag    =     element.findChild(\"ul\", {\"class\":\"PipeSepList Author\"})                           # 4 Auths\n",
    "                authors_tag =     authstag.find_all(\"li\")\n",
    "                authors_str = [author.getText().replace(\"\\n\",\"\") for author in authors_tag]\n",
    "\n",
    "                begi        =     miscel[0].getText().find(\"(\")\n",
    "                year        =     miscel[0].getText()[begi+1:begi+5]                                                # 5 Year\n",
    "\n",
    "                arts_db.append([title, abstract, DOI, journal, authors_str, year])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return arts_db;\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "# -query- returns a list of articles related to the search-term term*, note that this returns both IOP and NANO articles\n",
    "\n",
    "def query(terms):\n",
    "    all_arts = []\n",
    "\n",
    "    query_raw_iop  = PagesIOP(terms)\n",
    "    query_raw_nano = PagesNANO(terms)\n",
    "\n",
    "    IOP_DB         = stringifyIOP(query_raw_iop)\n",
    "    NANO_DB        = stringifyNANO(query_raw_nano)\n",
    "\n",
    "    for article in NANO_DB:\n",
    "        all_arts.append(article)\n",
    "    for article in IOP_DB:\n",
    "        all_arts.append(article)\n",
    "\n",
    "    return all_arts;\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "def search_kwords(arts, kwords):\n",
    "    candidates = []\n",
    "    count = 0\n",
    "    for art in arts:\n",
    "        if kwords in art[1]:\n",
    "            candidates.append(art)\n",
    "            count +=1\n",
    "    print(\"Coincidences: \" +str(count))\n",
    "    return candidates;\n",
    "\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\"\n",
    "\n",
    "\n",
    "def pandas_dataframe(all_arts, name):\n",
    "    test_arts = {}\n",
    "    for i in range(0,6):\n",
    "        ls = []\n",
    "        for j in range(0,len(all_arts)):\n",
    "            ls.append(all_arts[j][i])\n",
    "        if i == 0:\n",
    "            test_arts.update(Title        = ls)\n",
    "        elif i == 1:\n",
    "            test_arts.update(Abstract     = ls)\n",
    "        elif i == 2:\n",
    "            test_arts.update(DOI          = ls)\n",
    "        elif i == 3:\n",
    "            test_arts.update(Journal      = ls)\n",
    "        elif i == 4:\n",
    "            test_arts.update(Auths        = ls)\n",
    "        else:\n",
    "            test_arts.update(Year         = ls)\n",
    "    pd.DataFrame(test_arts).to_pickle(\"mispepinillos/{}.pkl\".format(name))\n",
    "    return pd.DataFrame(test_arts);\n",
    "\n",
    "# To read a pandas dataframe local_datarame= pandas.read_pickle(\"folder/file.pkl\")\n",
    "\n",
    "\"\"\"===============================================================================================================================\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python37064bitd82ff44cb29b4cc0b538f5e1ebc00dce"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}