{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "#\n",
    "#\n",
    "\n",
    "from urllib import request\n",
    "from time   import sleep\n",
    "from bs4    import BeautifulSoup\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Links for IOP search\n",
    "\n",
    "IOP_S  = \"https://iopscience.iop.org/nsearch?terms=\"\n",
    "IOP_E  = \"&nextPage=2&previousPage=-1&currentPage=\"\n",
    "IOP_P  = \"&searchDatePeriod=anytime&orderBy=relevance&pageLength=50\"\n",
    "\n",
    "# Links for NATURE search\n",
    "\n",
    "NANO_S = 'https://nano.nature.com/search?term=freeText%3A'\n",
    "NANO_E = '&sort-by=relevance&page-number='\n",
    "NANO_P = '&workflow=article'\n",
    "\n",
    "# /Souparticles : Returns a list of BeautifulSoup objects that contain articles\n",
    "\n",
    "# /material     : Is a str. containing the search term\n",
    "# /page         : The page in which we are searching\n",
    "# /max_try      : The maximum tries before givin up, commonly this is a result of a 400 status response from the server\n",
    "# /slpy         : Is the time between requests, recommended to prevent overloading requested server\n",
    "\n",
    "\n",
    "# Some pages can show problems if requested with -urllib- or -requests-, SwitchKitchen is implemented to\n",
    "# switch the way we obtain a BeautifulSoup object\n",
    "\n",
    "def SwitchKitchen(uri, kitchen = \"requests\", parse = \"lxml\"):\n",
    "    if   kitchen == \"requests\":\n",
    "         response = requests.get(uri)\n",
    "         soup     = BeautifulSoup(response.text, 'lxml')\n",
    "    elif kitchen == \"urllib\":\n",
    "         response = request.urlopen(uri)\n",
    "         soup     = BeautifulSoup(response, 'lxml')\n",
    "    else:\n",
    "        raise Exception(\"Problema en la elección del método mara obtener la respuesta del servidor.\")\n",
    "    return soup;\n",
    "\n",
    "def souparticlesIOP(query, page, max_int = 5, slpy = 3):\n",
    "    # At the beginning our IOP Soup is a None object\n",
    "    soupIOP = None\n",
    "    ints  = 0\n",
    "\n",
    "    while ints<max_int:    \n",
    "    # We try to catch some content from the server\n",
    "\n",
    "        try:\n",
    "            uri      = IOP_S+ query + IOP_E + str(page) + IOP_P\n",
    "            # If we catch a 200 response we can make a Beautifoul Soup (BS), parsing with lxml\n",
    "            soupIOP  = SwitchKitchen(uri,kitchen = \"urllib\")\n",
    "            break\n",
    "        except:\n",
    "            # If we catch an error, we can retry\n",
    "            print(\"Reintentando pag: \" + str(page))\n",
    "            sleep(slpy)\n",
    "            pass\n",
    "        finally:\n",
    "            ints +=1\n",
    "    \n",
    "    # For soupIOP == None this process needs to be repeated\n",
    "    if(soupIOP==None):\n",
    "        print( \"Por favor reinicie la busqueda para pag: \" + str(page))\n",
    "        return None;\n",
    "    else:\n",
    "        # Returning the articles in a BS object\n",
    "        print(\"Añadida pag: \" + str(page))\n",
    "        return soupIOP.find_all('div', {\"class\" : \"art-list-item-body\"})\n",
    "\n",
    "\n",
    "def souparticlesNANO(query, page, max_int = 5, slpy = 3):\n",
    "    nanosoup = None\n",
    "    ints=0\n",
    "    while ints<max_int:\n",
    "        try:\n",
    "            uri      = NANO_S+'\"'+query+'\"'+NANO_E+str(page)+NANO_P\n",
    "            nanosoup = SwitchKitchen(uri, kitchen = \"requests\")\n",
    "            break\n",
    "        except:\n",
    "            print(\"Reintentando pag: \" + str(page))\n",
    "            sleep(slpy)\n",
    "            pass\n",
    "        finally:\n",
    "            ints +=1\n",
    "    if(nanosoup==None):\n",
    "        print( \"Por favor reinicie la busqueda para pag: \" + str(page))\n",
    "        return None;\n",
    "    else:\n",
    "        print(\"Añadida pag: \" + str(page))\n",
    "        return nanosoup.find_all('li', {\"class\" : \"Results_listItem\" })\n",
    "\n",
    "def PagesIOP(query, end=12):\n",
    "    all_     = []\n",
    "    to_retry = []\n",
    "    for i in range(1,end):\n",
    "        item = souparticlesIOP(query, i)\n",
    "        if(isinstance(item, str)):\n",
    "            to_retry.append(i)\n",
    "        else:\n",
    "            all_.append(item)\n",
    "    return [all_, to_retry];\n",
    "\n",
    "def PagesNANO(query, end=12):\n",
    "    all_     = []\n",
    "    to_retry = []\n",
    "    for i in range(1,end):\n",
    "        item = souparticlesNANO(query, i)\n",
    "        if(isinstance(item, str)):\n",
    "            to_retry.append(i)\n",
    "        else:\n",
    "            all_.append(item)\n",
    "    return [all_, to_retry];\n",
    "\n",
    "def stringify(raw_pages):\n",
    "    arts_db = []\n",
    "    for page in raw_pages:\n",
    "        for element in page:\n",
    "            try:\n",
    "                title     =     element.findChild('h2' , {\"class\"   : \"art-list-item-title\"}).findChild(\"a\")          # 0 title\n",
    "                abstract  =     element.findChild('div', {\"class\"   : \"article-text view-text-small\"}).findChild(\"p\") # 1 abstract\n",
    "                DOI       =     element.findChild('a'  , {\"class\"   : \"mr-2\"})                                        # 2 DOI\n",
    "                journal   =     element.findChild('em')                                                               # 3 Journal\n",
    "                vol       =     element.findChild('b')                                                                # 4 vol\n",
    "                authors   =     element.find_all('span', {\"itemprop\": \"author\"})                                      # 5 auths *beta*\n",
    "\n",
    "                yearfind  =     element.find_all(\"p\"   , { \"class\" :\"small art-list-item-meta\"})                      # 6 year\n",
    "                year      =     int(yearfind[1].text[14:19])\n",
    "\n",
    "                arts_db.append([title.text ,abstract.text, DOI.text, journal.text, vol.text ,[auth.getText() for auth in authors], year])\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    return arts_db;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Añadida pag: 1\nAñadida pag: 1\n"
    }
   ],
   "source": [
    "a1 = souparticlesIOP(\"ZnTe\", 1)\n",
    "a2 = souparticlesNANO(\"ZnTe\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "50"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(a1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def panda_to_me(all_arts):\n",
    "    test_arts = {}\n",
    "    for i in range(0,7):\n",
    "        ls = []\n",
    "        for j in range(0,len(all_arts)):\n",
    "            ls.append(all_arts[j][i])\n",
    "        if i == 0:\n",
    "            test_arts.update(Title        = ls)\n",
    "        elif i == 1:\n",
    "            test_arts.update(Abstract     = ls)\n",
    "        elif i == 2:\n",
    "            test_arts.update(DOI          = ls)\n",
    "        elif i == 3:\n",
    "            test_arts.update(Journal      = ls)\n",
    "        elif i == 4:\n",
    "            test_arts.update(Vol          = ls)\n",
    "        elif i == 5:\n",
    "            test_arts.update(Auths        = ls)\n",
    "        else:\n",
    "            test_arts.update(Year         = ls)\n",
    "    return pd.DataFrame(test_arts);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZnTe_db    = stringify(ZnTe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZnTe_panda = panda_to_me(ZnTe_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZnTe_panda.to_pickle(\"mispepinillos/ZnTe_DB.pkl\")"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}